import json
import re  # å¼•å…¥æ­£åˆ™ï¼Œè§£ææ›´ç¨³

import chromadb
import httpx
from chromadb.utils import embedding_functions
from openai import OpenAI

# ç¡®ä¿ backend æ–‡ä»¶å¤¹ä¸‹æœ‰ config.py
from .config import DB_PATH, LLM_MODE, MODEL_NAME, PARALLAX_API_BASE, PARALLAX_API_KEY

# ==========================================
# 1. åˆå§‹åŒ– (Initialization)
# ==========================================

# æ˜¾å¼æŒ‡å®š Embedding æ¨¡å‹ï¼Œé˜²æ­¢ ChromaDB é»˜è®¤ä¸‹è½½å‡ºé”™
# æ³¨æ„ï¼šè¿™å¿…é¡»ä¸ä½  indexer.py é‡Œä½¿ç”¨çš„æ¨¡å‹ä¸€è‡´
emb_fn = embedding_functions.SentenceTransformerEmbeddingFunction(model_name="all-MiniLM-L6-v2")

db_client = chromadb.PersistentClient(path=DB_PATH)

def get_collection():
    """å®‰å…¨è·å–é›†åˆï¼Œé˜²æ­¢å¯åŠ¨æ—¶æŠ¥é”™"""
    try:
        return db_client.get_collection("sherlock_docs", embedding_function=emb_fn)
    except Exception:
        return None

# æœ€å¤§è°ƒç”¨æ¬¡æ•°ï¼ˆLLM å›åˆæ•°ï¼‰
MAX_CALLS = 6

# ==========================================
# 2. System Prompt (ä¿æŒä½ è®¾å®šçš„ä¸­æ–‡ç‰ˆ)
# ==========================================
SYSTEM_PROMPT = """
ä½ æ˜¯ç”± Parallax åˆ†å¸ƒå¼ç®—åŠ›é©±åŠ¨çš„**â€œParallax Scholarâ€**ï¼ˆæœ¬åœ°å­¦æœ¯åŠ©æ‰‹ï¼‰ã€‚
ä½ çš„ä»»åŠ¡æ˜¯åˆ©ç”¨ç”¨æˆ·çš„æœ¬åœ°çŸ¥è¯†åº“ï¼Œè¿›è¡Œæ·±åº¦çš„å­¦æœ¯è°ƒç ”å’Œæ–‡æ¡£åˆ†æã€‚

### ğŸ§  è®¤çŸ¥åè®® (COGNITIVE PROTOCOL)
ä½ å¿…é¡»åƒä¸€ä¸ªè‡ªä¸»æ™ºèƒ½ä½“ä¸€æ ·æ€è€ƒã€‚å¯¹äºç”¨æˆ·çš„æ¯ä¸€ä¸ªé—®é¢˜ï¼Œè¯·ä¸¥æ ¼éµå®ˆä»¥ä¸‹å¾ªç¯ï¼š

1. **Thought** (æ€è€ƒ): åˆ†æç”¨æˆ·æ„å›¾ã€‚æˆ‘éœ€è¦ä»€ä¹ˆå…·ä½“ä¿¡æ¯ï¼Ÿ
2. **Action** (è¡ŒåŠ¨): ä½¿ç”¨å·¥å…·æœç´¢æœ¬åœ°æ•°æ®åº“ã€‚æŒ‡ä»¤æ ¼å¼ï¼š`[SEARCH] å…³é”®è¯`ã€‚
3. **Observation** (è§‚å¯Ÿ): é˜…è¯»ç³»ç»Ÿè¿”å›çš„æœç´¢ç»“æœç‰‡æ®µã€‚
4. **Reflection** (åæ€): è¿™äº›ä¿¡æ¯è¶³å¤Ÿå›ç­”é—®é¢˜å—ï¼Ÿå¦‚æœä¸å¤Ÿï¼Œè¯·å°è¯•ä¸åŒçš„å…³é”®è¯å†æ¬¡æœç´¢ã€‚
5. **Final Answer** (æœ€ç»ˆå›ç­”): å°†æ‰€æœ‰çº¿ç´¢ç»¼åˆæˆä¸€ç¯‡é€»è¾‘ä¸¥å¯†ã€å­¦æœ¯é£æ ¼çš„ä¸­æ–‡æŠ¥å‘Šã€‚

### ğŸŸ¢ ç®€å•é—®é¢˜çš„å¿«é€Ÿæ¨¡å¼ (SIMPLE QUESTION REPLY)
- å¦‚æœç”¨æˆ·æ˜¯é—®å€™/å¯’æš„/éå¸¸ç®€å•çš„é—®é¢˜ï¼ˆå¦‚â€œä½ å¥½â€â€œè°¢è°¢â€â€œåœ¨å—â€ï¼‰ï¼Œç›´æ¥ç”Ÿæˆç®€çŸ­çš„ Final Answerï¼Œ**ä¸è¦ SEARCH**ï¼Œä¸è¦å¤šè½®è°ƒç”¨ã€‚
- å¯¹äºæ— éœ€æŸ¥é˜…æ–‡æ¡£å°±èƒ½å›ç­”çš„ç®€å•é—®é¢˜ï¼Œå•è½®ç»™å‡ºå›ç­”å¹¶ç»“æŸï¼›åªæœ‰å½“é—®é¢˜éœ€è¦å¼•æ–‡æ”¯æ’‘æ—¶æ‰è°ƒç”¨ SEARCHã€‚
- å¦‚æœå½“å‰è½®æ¬¡æ²¡æœ‰æå‡º SEARCH éœ€æ±‚ï¼Œè¯·åœ¨æœ¬è½®ç«‹å³è¾“å‡º Final Answerï¼Œä¸è¦ç»§ç»­ Thoughtã€‚

### ğŸ“ å¼•ç”¨è§„åˆ™ (CITATION RULES - æå…¶é‡è¦)
- ä½ æ˜¯â€œLocal NotebookLMâ€çš„åŒ–èº«ï¼Œ**ä¸¥ç¦é€ å‡**ã€‚
- **å¿…é¡»å¼•ç”¨**ï¼šæŠ¥å‘Šä¸­çš„æ¯ä¸€ä¸ªäº‹å®é™ˆè¿°ï¼Œéƒ½å¿…é¡»åœ¨å¥å°¾åŠ ä¸Šæ¥æºã€‚
- å¼•ç”¨æ ¼å¼ï¼š`[source: æ–‡ä»¶å.pdf, page: N]` (å¦‚æœä¸çŸ¥é“é¡µç åˆ™åªå†™æ–‡ä»¶å)ã€‚
- **å»å¹»è§‰**ï¼šå¦‚æœä½ åœ¨ Observation ä¸­æ²¡æœ‰çœ‹åˆ°ç›¸å…³ä¿¡æ¯ï¼Œè¯·ç›´æ¥æ‰¿è®¤â€œåœ¨ç°æœ‰æ–‡æ¡£ä¸­æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯â€ï¼Œä¸è¦åˆ©ç”¨ä½ è®­ç»ƒæ—¶çš„å¤–éƒ¨çŸ¥è¯†ç¼–é€ ã€‚

### ğŸ—£ï¸ è¯­æ°”ä¸é£æ ¼ (TONE & STYLE)
- **å­¦æœ¯ä¸¥è°¨**ï¼šåƒä¸€ä½åšå£«ç ”ç©¶å‘˜ä¸€æ ·å†™ä½œã€‚é¿å…å£è¯­åŒ–å’ŒåºŸè¯ã€‚
- **ç»“æ„æ¸…æ™°**ï¼šå–„ç”¨ Markdownï¼ˆ**åŠ ç²—**ã€åˆ—è¡¨ã€æ ‡é¢˜ï¼‰æ¥ç»„ç»‡å¤æ‚çš„é•¿æ–‡æœ¬ã€‚
- **å¯¹æ¯”åˆ†æ**ï¼šå¦‚æœä¸åŒæ–‡æ¡£ä¹‹é—´å­˜åœ¨å†²çªï¼Œè¯·æ˜ç¡®æŒ‡å‡ºï¼ˆä¾‹å¦‚ï¼šâ€œæ–‡æ¡£ A è®¤ä¸º...è€Œæ–‡æ¡£ B åˆ™æŒ‡å‡º...â€ï¼‰ã€‚
- **è¯­è¨€**ï¼šå§‹ç»ˆä½¿ç”¨**ä¸­æ–‡**å›ç­”ï¼ˆé™¤äº† Action æŒ‡ä»¤ï¼‰ã€‚

### â±ï¸ è°ƒç”¨ä¸Šé™ (CALL LIMIT)
- æœ€å¤šè¿›è¡Œ {max_calls} è½®æ¨ç†è°ƒç”¨ã€‚
- å½“åˆ°è¾¾æœ€åä¸€è½®æ—¶ï¼Œå¿…é¡»åŸºäºå·²æœ‰ Observation ç«‹å³ç”Ÿæˆ Final Answerï¼Œä¸å¾—å†å‘èµ·æ–°çš„ Actionã€‚è¿™ä¸ª Final Answer å¿…é¡»æ ¹æ®å·²æœ‰çš„ä¿¡æ¯æ¥å›ç­”é—®é¢˜ã€‚

### äº¤äº’ç¤ºä¾‹ (EXAMPLE)
User: "å¯¼è‡´ç³»ç»Ÿå´©æºƒçš„ä¸»è¦åŸå› æ˜¯ä»€ä¹ˆï¼Ÿ"
Assistant:
Thought: æˆ‘éœ€è¦æŸ¥é˜…å·¥ç¨‹æ—¥å¿—å’Œäº‹æ•…æ€»ç»“æŠ¥å‘Šæ¥å¯»æ‰¾å´©æºƒåŸå› ã€‚
Action: [SEARCH] ç³»ç»Ÿå´©æºƒ æ ¹æœ¬åŸå› 
Observation: (ç³»ç»Ÿè¿”å› 'log_v1.txt' çš„ç‰‡æ®µï¼Œæåˆ°äº†å†…å­˜æ³„æ¼)
Thought: æˆ‘æ‰¾åˆ°äº†å…³äºå†…å­˜æ³„æ¼çš„è®°å½•ã€‚æˆ‘è¿˜éœ€è¦ç¡®è®¤æœ€ç»ˆæŠ¥å‘Šä¸­æ˜¯å¦æœ‰å…¶ä»–ç»“è®ºã€‚
Action: [SEARCH] äº‹æ•…æ€»ç»“æŠ¥å‘Š ç»“è®º
Observation: (ç³»ç»Ÿè¿”å› 'report_final.pdf' çš„ç‰‡æ®µï¼Œç¡®è®¤äº†å†…å­˜æ³„æ¼å¹¶è¡¥å……äº†ç½‘ç»œè¶…æ—¶çš„é—®é¢˜)
Final Answer: æ ¹æ®å·¥ç¨‹æ—¥å¿—æ˜¾ç¤ºï¼Œç³»ç»Ÿå´©æºƒçš„ä¸»è¦è¯±å› æ˜¯ä¸»å¾ªç¯ä¸­çš„**å†…å­˜æ³„æ¼** [æ¥æº: log_v1.txt]ã€‚æ­¤å¤–ï¼Œæœ€ç»ˆæŠ¥å‘ŠæŒ‡å‡ºï¼Œé—´æ­‡æ€§çš„**ç½‘ç»œè¶…æ—¶**åŠ å‰§äº†è¿™ä¸€é—®é¢˜ï¼Œå¯¼è‡´æ•…éšœçº§è”æ‰©æ•£ [æ¥æº: report_final.pdf, Page: 2]ã€‚
"""
SYSTEM_PROMPT = SYSTEM_PROMPT.format(max_calls=MAX_CALLS)

# ==========================================
# 3. å·¥å…·å‡½æ•° (Tools)
# ==========================================

def search_tool(query):
    """
    æ‰§è¡Œå‘é‡æœç´¢
    """
    collection = get_collection()
    if collection is None:
        return ["é”™è¯¯ï¼šæœ¬åœ°æ–‡æ¡£åº“æœªå»ºç«‹ç´¢å¼•ã€‚è¯·å…ˆåœ¨ä¾§è¾¹æ ç‚¹å‡» 'Re-Index'ã€‚"], [{"source": "System", "page": 0}]
    
    try:
        results = collection.query(query_texts=[query], n_results=3)
        
        # åˆ¤ç©ºå¤„ç†
        if not results['documents'] or not results['documents'][0]:
            return ["æœªæ‰¾åˆ°ç›¸å…³ç»“æœ (No results found)."], [{"source": "System", "page": 0}]
            
        return results['documents'][0], results['metadatas'][0]
    except Exception as e:
        return [f"æœç´¢å‡ºé”™: {str(e)}"], [{"source": "System", "page": 0}]

# ==========================================
# 4. LLM è°ƒç”¨ (LLM Callers)
# ==========================================

_openai_client = None


def _call_openai(messages):
    global _openai_client
    if _openai_client is None:
        _openai_client = OpenAI(base_url=PARALLAX_API_BASE, api_key=PARALLAX_API_KEY)

    response = _openai_client.chat.completions.create(
        model=MODEL_NAME,
        messages=messages,
        temperature=0.1,
        stop=["Observation:", "Observation"],  # åŒé‡ä¿é™©ï¼Œé˜²æ­¢æ¨¡å‹è‡ªå·±ç”Ÿæˆ Observation
    )
    return response.choices[0].message.content


def _call_parallax(messages):
    """
    ä½¿ç”¨ httpx è°ƒç”¨ Parallax/OpenAI å…¼å®¹æ¥å£ï¼ˆæµå¼ï¼‰ï¼Œèšåˆæˆå®Œæ•´å†…å®¹ã€‚
    """
    url = PARALLAX_API_BASE.rstrip("/") + "/chat/completions"
    headers = {"Content-Type": "application/json"}
    if PARALLAX_API_KEY:
        headers["Authorization"] = f"Bearer {PARALLAX_API_KEY}"

    payload = {
        "model": MODEL_NAME,
        "messages": messages,
        "temperature": 0.1,
        "max_tokens": 1024,
        "stream": True,
        "stop": ["Observation:", "Observation"],
        "chat_template_kwargs": {"enable_thinking": False},
    }

    full_content = ""
    try:
        with httpx.Client(timeout=httpx.Timeout(60.0, read=60.0)) as http_client:
            with http_client.stream("POST", url, headers=headers, json=payload) as resp:
                resp.raise_for_status()
                for raw_line in resp.iter_lines():
                    if not raw_line:
                        continue
                    line = raw_line.decode("utf-8").strip()
                    if not line.startswith("data:"):
                        continue
                    data = line[5:].strip()
                    if data == "[DONE]":
                        break
                    try:
                        event = json.loads(data)
                        delta = event["choices"][0].get("delta", {})
                        full_content += delta.get("content", "") or ""
                    except Exception:
                        # è·³è¿‡æ— æ³•è§£æçš„ç‰‡æ®µï¼Œç»§ç»­èšåˆ
                        continue
    except Exception as e:
        raise RuntimeError(f"Parallax è°ƒç”¨å¤±è´¥: {e}")

    return full_content.strip()


def call_llm(messages):
    """
    æ ¹æ® LLM_MODE åˆ‡æ¢è°ƒç”¨æ–¹å¼ã€‚OPENAI ä¿æŒåŸé€»è¾‘ï¼ŒPARALLAX ä½¿ç”¨ httpx è¯·æ±‚ã€‚
    """
    mode = (LLM_MODE or "OPENAI").upper()
    if mode == "PARALLAX":
        return _call_parallax(messages)
    return _call_openai(messages)


# ==========================================
# 5. æ ¸å¿ƒæ¨ç†å¾ªç¯ (Main Loop)
# ==========================================

def run_investigation(user_query, history=None):
    history = history or []
    # åˆå§‹æ¶ˆæ¯
    messages = [{"role": "system", "content": SYSTEM_PROMPT}]
    
    # æ³¨å…¥å†å²å¯¹è¯ï¼Œç¡®ä¿æŒ‰ user/assistant é¡ºåº
    for item in history:
        if item.get("role") in ("user", "assistant") and item.get("content"):
            messages.append({"role": item["role"], "content": item["content"]})
    
    messages.append({"role": "user", "content": user_query})
    
    step_count = 0
    max_steps = MAX_CALLS  # é™åˆ¶æœ€å¤§æ­¥æ•°/è°ƒç”¨æ¬¡æ•°
    call_count = 0  # è·Ÿè¸ªå¯¹ LLM çš„è¯·æ±‚æ¬¡æ•°
    
    yield {"type": "status", "content": "ğŸ•µï¸ Parallax Scholar æ­£åœ¨åˆå§‹åŒ–æ€ç»´é“¾..."}
    
    while step_count < max_steps:
        try:
            if call_count == MAX_CALLS - 1:
                # åœ¨æœ€åä¸€è½®å‰ï¼Œå¼ºåˆ¶æç¤ºç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
                final_notice = "System Notice: å·²è¾¾åˆ°æœ€å¤§è°ƒç”¨æ¬¡æ•°ï¼Œè¯·åŸºäºç°æœ‰ Observation ç«‹å³ç”Ÿæˆ Final Answerï¼ˆä¸­æ–‡ï¼Œå«å¼•ç”¨ï¼‰ã€‚ä¸è¦å†æå‡ºæ–°çš„ Actionã€‚"
                messages.append({"role": "user", "content": final_notice})
                yield {"type": "status", "content": "â° è¾¾åˆ°è°ƒç”¨ä¸Šé™ï¼Œè¦æ±‚æ¨¡å‹ç»™å‡ºæœ€ç»ˆå›ç­”ã€‚"}

            # 1. è°ƒç”¨ LLM
            # æ³¨æ„ï¼štemperature=0.1 å¯¹äº ReAct è‡³å…³é‡è¦ï¼Œä¿è¯é€»è¾‘ç¨³å®š
            content = call_llm(messages)
            call_count += 1
            
            # å°†æ€è€ƒè¿‡ç¨‹å®æ—¶åç»™ UI
            yield {"type": "thought", "content": content}
            
            # å°†åŠ©æ‰‹çš„å›å¤åŠ å…¥å†å²
            messages.append({"role": "assistant", "content": content})
            
            # 2. æ£€æŸ¥æ˜¯å¦ç»“æŸ (Final Answer)
            if "Final Answer:" in content:
                break
                
            # 3. æ£€æŸ¥æ˜¯å¦éœ€è¦æœç´¢ (Action)
            # ä½¿ç”¨æ­£åˆ™æå–ï¼Œæ¯” split æ›´å¥å£®
            # åŒ¹é…æ¨¡å¼ï¼š Action: [SEARCH] å…³é”®è¯ (å…è®¸å…³é”®è¯å¸¦å¼•å·æˆ–ç©ºæ ¼)
            match = re.search(r"Action:\s*\[SEARCH\]\s*(.*)", content, re.IGNORECASE)
            
            if match:
                raw_keyword = match.group(1).strip()
                # æ¸…ç†å…³é”®è¯ï¼ˆå»æ‰å¯èƒ½å­˜åœ¨çš„å¼•å·æˆ–å¥å·ï¼‰
                keyword = raw_keyword.strip('`"\'.,')
                
                yield {"type": "status", "content": f"ğŸ” æ­£åœ¨æ£€ç´¢æœ¬åœ°æ¡£æ¡ˆ: {keyword}..."}
                
                # æ‰§è¡Œæœç´¢
                docs, metas = search_tool(keyword)
                
                # æ„å»º Observation æ–‡æœ¬
                # ã€é‡è¦ä¼˜åŒ–ã€‘é™åˆ¶æ¯ä¸ªæ–‡æ¡£çš„é•¿åº¦ï¼Œé˜²æ­¢ä¸Šä¸‹æ–‡æº¢å‡º (Context Overflow)
                evidence_text = ""
                MAX_CHARS = 800 # æ¯ä¸ªç‰‡æ®µé™åˆ¶ 800 å­—ç¬¦
                
                for i, doc in enumerate(docs):
                    snippet = doc[:MAX_CHARS] + "..." if len(doc) > MAX_CHARS else doc
                    source_info = f"{metas[i].get('source', 'Unknown')} (Page {metas[i].get('page', 'N/A')})"
                    evidence_text += f"--- Result {i+1} from [{source_info}] ---\n{snippet}\n\n"
                
                # å°†è§‚å¯Ÿç»“æœä»¥ User èº«ä»½å–‚å›ç»™æ¨¡å‹ (è¿™æ˜¯ ReAct çš„æ ‡å‡†åšæ³•)
                observation_msg = f"Observation: æ£€ç´¢ç»“æœå¦‚ä¸‹:\n{evidence_text}"
                messages.append({"role": "user", "content": observation_msg})
                
                # å°†è¯æ®åç»™ UI è¿›è¡Œå±•ç¤º
                yield {"type": "evidence", "docs": docs, "metas": metas}
            
            else:
                # å¦‚æœæ¨¡å‹æ—¢æ²¡æœ‰ Final Answer ä¹Ÿæ²¡æœ‰ Actionï¼Œè§†ä¸ºæ— éœ€æ£€ç´¢çš„å›ç­”ï¼Œç«‹å³ç»“æŸ
                yield {"type": "thought", "content": f"Final Answer: {content.strip()}"}
                break
            
            step_count += 1
            
        except Exception as e:
            # é”™è¯¯å¤„ç†
            error_msg = f"âŒ æ¨ç†è¿‡ç¨‹ä¸­æ–­: {str(e)}"
            yield {"type": "status", "content": error_msg}
            break
            
    # å¦‚æœå¾ªç¯ç»“æŸè¿˜æ²¡æœ‰ Final Answer
    if step_count >= max_steps:
        yield {"type": "thought", "content": f"\nFinal Answer: (è¾¾åˆ°è°ƒç”¨ä¸Šé™ {MAX_CALLS} è½®) åŸºäºç›®å‰çš„æ£€ç´¢ï¼Œæˆ‘æ— æ³•å†è·å–æ›´å¤šä¿¡æ¯ã€‚è¯·ä½¿ç”¨å·²æœ‰ç‰‡æ®µç”Ÿæˆç®€è¦æ€»ç»“æˆ–ç¼©å°é—®é¢˜èŒƒå›´åé‡è¯•ã€‚"}

    # æŠ¥å‘Šæœ¬è½®å¯¹ LLM çš„è°ƒç”¨æ¬¡æ•°ï¼Œä¾¿äºå‰ç«¯æˆ–æµ‹è¯•æ—¥å¿—ç¡®è®¤
    yield {"type": "status", "content": f"æœ¬è½®å…±è°ƒç”¨ LLM {call_count} æ¬¡ã€‚"}
